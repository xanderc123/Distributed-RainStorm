Here is the organized command set with English comments/annotations, structured according to the MP4 Demo Flow. You can copy this directly into your notes or a script.

ðŸŸ¢ Phase 1: Cleanup & Setup (Run on all VMs)
1. Clean Environment

Bash

# Kill all running RainStorm and generic Python processes to ensure a clean state
pkill -9 -f rainstorm; pkill -9 -f python3

# Remove old logs (Leader/Worker logs, Task logs) to clear history
rm -f leader.log worker.log task_*.log leader_console.log worker_console.log source_*.log
2. Start Membership Service

Bash

# Start Membership Protocol (Introducer/Member) in the background
nohup python3 membership.py --port 9000 --introducer fa25-cs425-9801.cs.illinois.edu:9000 --mode pingack --drop 0.0 --t_fail 2.5 --t_suspect 1.9 --t_cleanup 2 > membership_bg.log 2>&1 &

# (Optional) Verify membership list IDs
python control.py list_mem_ids

# (Optional) Kill membership if needed
pkill -f membership.py
3. Start RainStorm Cluster

Bash

# [On VM1] Start the Leader process
nohup python3 rainstorm_daemon.py --mode leader --logfile leader.log > leader_console.log 2>&1 &
nohup python3 rainstorm_daemon.py --mode worker --logfile worker.log > worker_console.log 2>&1 &

# [On VM1, VM2, VM3, VM4] Start the Worker process (VM1 runs both)
nohup python3 rainstorm_daemon.py --mode worker --logfile worker.log < /dev/null > worker_console.log 2>&1 &

# (Emergency) Kill Daemon only
pkill -f rainstorm_daemon.py
ðŸ§ª Phase 2: Test 0 - Basic Functionality (Identity)
Goal: Verify throughput and data integrity without data loss.

Bash

# Submit Job: 1 Stage, 3 Tasks/Stage, Identity Operator, Rate=100 tuples/sec
python3 rainstorm_client.py 1 3 identity "" dataset1.csv output_test0.txt true false 100

# 1. Check processing rate during execution (Should stay around 100)
tail -f leader.log

# 2. Verify Data Integrity after completion
# Count lines in original file
wc -l dataset1.csv

# Count lines in HyDFS output (Should be Source_Lines - 1 Header)
cat DFS/output_test0.txt/* | wc -l
ðŸ§ª Phase 3: Test 1 - Correctness (Filter & Count)
Goal: Verify that the logic for filtering and stateful aggregation is correct.

Bash

# Submit Job: 2 Stages, 3 Tasks/Stage, Filter "Sign" -> Aggregate Col 6, Rate=50
python3 rainstorm_client.py 2 3 filter "Sign" aggregate 6 dataset1.csv output_test1.txt true false 50

# 1. Verify RainStorm Result (Check the final accumulated counts at the end)
cat DFS/output_test1.txt/* | tail -n 20

# 2. Verify against Ground Truth (Run Python script to calculate locally)
python3 -c "import csv, sys; [print(next(csv.reader([line]))[6]) for line in sys.stdin if 'Sign' in line]" < dataset1.csv | sort | uniq -c
ðŸ§ª Phase 4: Test 2 - Fault Tolerance (Kill Task)
Goal: Verify the system detects a failure and restarts the task automatically.

Bash

# 1. Submit Job: Same as Test 1, but output to a new file
python3 rainstorm_client.py 2 3 filter "Sign" aggregate 6 dataset1.csv output_test2.txt true false 50

# 2. List running tasks to find a victim (Look for Stage 1 Filter PID)
python3 rainstorm_client.py list_tasks

# 3. Kill the specific task (Replace <VM_HOSTNAME> and <PID> with actual values)
python3 rainstorm_client.py kill_task <VM_HOSTNAME> <PID>

# 4. Monitor Leader Log for Recovery
# You should see "[Failure]..." followed by "[Recovery] Restarting Task..."
tail -f leader.log
ðŸ§ª Phase 5: Test 3 - Autoscaling (App 2)
Goal: Verify the system scales up when load > HW (High Watermark).

Bash

# Submit Job: App 2 (Filter + Transform), Autoscale=True, Rate=1000, HW=5 (Low threshold to trigger scale up)
python3 rainstorm_client.py 2 1 filter "Sign" transform "cut1-3" dataset2.csv output_test3.txt false true 1000 2 5

# 1. Monitor Leader Log for "Scaling UP" events
tail -f leader.log

# 2. Check Worker Logs (Optional)
tail -f worker.log

# 3. Check HyDFS output (Verify content format is transformed/cut)
ls -lh DFS/
head -n 5 DFS/output_test3.txt/block-00001

# 4. Check Processes (Verify number of python processes increased)
ps -ef | grep rainstorm
ðŸ“‚ Phase 6: HyDFS Debugging (Optional Helper Commands)
If you need to debug the file system:

Bash

# Get a file from HyDFS to local
python3 hydfs_client.py --hosts hosts.txt get output_test.txt get_result.txt

# List files/blocks in HyDFS
python3 hydfs_client.py --hosts hosts.txt ls output_test.txt